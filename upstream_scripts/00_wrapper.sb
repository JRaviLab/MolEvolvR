#!/bin/bash

# MolEvolvR: Companion wrapper script
# accnum –> fasta –> deltablast -> edirect -> blastclust -> iprscan/rpsblast -> cleanup w/ lineages!
# To run on compute.cvm.msu.edu

# Created: 2020.07.09
# Last modified: 2020.11.13
# Authors: Lauren Sosinski, Janani Ravi
# On GitHub: currently in jravilab/laurensosinski

############
## TORQUE ##
############
#PBS -l walltime=04:30:00	# time limit; RESET, if you need longer time
#PBS -l nodes=1:ppn=10		# number of nodes requested
#PBS -m abe			# email notifications for job
#PBS -M=sosinsk7@msu.edu	# user email; RESET
#PBS -F arguments		# allows script to accept argument input
#PBS -t 1-3			# number of identical jobs to submit to the queue
#PBS -N molevol_analysis	# name of job being run

## print start/stop printf in individual scripts

## change output directory based on user input
OUTPATH=$PBS_O_WORKDIR
cd ${OUTPATH}

## !! NOTES FOR LS ##
## add flags/help function after finishing script and/or gencontext
## figure out where input list is coming from, create output folder in same directory 
## talk to Sam about how this works with Pins package

## USER INPUTS
INFILEPATHS_LIST=$1
DB=refseq
NHITS=5000
EVAL=1e-5

# Location of databases/dependencies 
export BLASTDB=/data/common_data/blastdb/v5:/data/common_data/blastdb/ncbidb:/data/common_data/blastdb:/data/common_data/blastdb/FASTA:$BLASTDB
export BLASTMAT=/opt/software/BLAST/2.2.26/data
export INTERPRO=/opt/software/iprscan/5.47.82.0-Python3/data:/data/common_data/iprscan:$INTERPRO
export NCBI_API_KEY=882b28aa19ece4679d4fa5adcf3319f5df09

#####################
## LOADING MODULES ##
#####################

module purge 					## clear loaded modules
module load R		 			## load R
module load edirect 				## load edirect
module load BLAST				## load blast (for blastclust)
module load BLAST+ BioPerl 			## load blast+
module load iprscan 				## load iprscan

start=$SECONDS 					## get current time
DT=$(date '+%d/%m/%Y %H:%M:%S')

FILES=$(cat ${INFILEPATHS_LIST})			## list of files (with paths) to be processed
#total=$(( ((3*60)+50)*60 ))			## total time the job can take in seconds, this should match your SBATCH line above
#maxtime=$(( 120*60 )) 				## maximum time to process one input, need to do some experimenting with your inputs

for FILE in $(shuf -e ${FILES[@]})
do
   SUFFIX=$(basename -s .fa $FILE)	## takes SUFFIX of file
   OUTDIR=${OUTPATH}/${SUFFIX}_out	## variable containing output filepath based SUFFIX
   printf "${SUFFIX}"

   if [ ! -d ${OUTDIR} ]; then 			## if the output directory doesn't exist
      mkdir ${OUTDIR}				## make the directory

      ## DELTABLAST ##
      db_start=$SECONDS
      sh /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/01.1_deltablast.sh $FILE $SUFFIX $OUTDIR $DB $NHITS $EVAL
      db_duration=$(( $SECONDS - $db_start ))
      
      ## BLAST RESULT CLEANUP ##
      db_cln_start=$SECONDS
      Rscript /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/01.2_cleanup_clblast.R ${OUTDIR}/${SUFFIX}.${DB}.${EVAL}.txt
      db_cln_duration=$(( $SECONDS - $db_cln_start ))

      ## ACC2FA -- getting fasta FILES for deltablast output(s) 
      acc2fa_start=$SECONDS
      sh /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/02_acc2fa.sh ${OUTDIR}/${SUFFIX}.${DB}.${EVAL}.txt $SUFFIX $OUTDIR
      acc2fa_duration=$(( $SECONDS - $acc2fa_start ))

      ## BLASTCLUST ##
      bclust_start=$SECONDS
      sh /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/03.1_blastclust.sh ${OUTDIR}/${SUFFIX}.all_accnums.fa $SUFFIX $OUTDIR
      bclust_duration=$(( $SECONDS - $bclust_start ))

      ## CLUST2TABLE
      c2t_start=$SECONDS
      Rscript /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/03.2_clust2table.R ${OUTDIR}/${SUFFIX}.all_accnums.bclust.L60S80.out ${OUTDIR}/${SUFFIX}.${DB}.${EVAL}.cln.txt
      c2t_duration= $(( $SECONDS - $c2t_start ))

      ## INTERPROSCAN ##
      ## add second run for original protein, too
      ipr_start=$SECONDS
      sh /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/04a_iprscan.sh ${OUTDIR}/${SUFFIX}.all_accnums.fa ${SUFFIX} ${OUTDIR}
      ipr_duration=$(( $SECONDS - $ipr_start ))

      ## IPR2DA ##
      ipr2da_start=$SECONDS
      Rscript /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/05a_ipr2da.R ${OUTDIR}/${SUFFIX}.iprscan.tsv ${OUTDIR}/${SUFFIX}.${DB}.${EVAL}.cln.txt #${SUFFIX}
      ipr2da_duration=$(( $SECONDS - $ipr2da_start ))

      ## RPSBLAST ## 		
      rps_start=$SECONDS
      sh /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/04b_rpsblast.sh ${OUTDIR}/${SUFFIX}.all_accnums.fa ${SUFFIX} ${OUTDIR}
      rps_duration=$(( $SECONDS - $rps_start))

      ## RPS2DA ##
      rps2da_start=$SECONDS
      Rscript /data/research/jravilab/laurensosinski/scripts/wrapper-scripts/05b_rps2da.R ${OUTDIR}/${SUFFIX}.rps.out ${OUTDIR}/${SUFFIX}.${DB}.${EVAL}.cln.txt
      rps2da_duration=$(( $SECONDS - $rps2da_start ))

      ## Add benchmarking times to logfile
      printf "\n${DT}\t${SUFFIX}\t${db_duration}\t${db_cln_duration}\t${acc2fa_duration}\t${bclust_duration}\t${c2t_duration}\t${ipr_duration}\t${ipr2da_duration}\t${rps_duration}\t${rps2da_duration}" >> molevolvr_logfile.txt

      cp ${file} ${OUTDIR}	# copy fasta file to output directory
      
      chmod -R ug+wr ${OUTDIR}

      ## Figure out how long the entire script took to run
      duration=$(( $SECONDS - $start ))
      printf "\nTotal run time: $duration\n"

      ## And how much time is left
      timeleft=$(( $total - $duration ))

      ## If there's a chance we get a long input to process, then
      ## resubmit this job, then kill this job
 #     if [ ${timeleft} -lt ${maxtime} ]; then
  #       sbatch --array=${SLURM_ARRAY_TASK_ID} $0
   #      scancel ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}
    #  fi
   fi
done
