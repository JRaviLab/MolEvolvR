#!/bin/bash

# MolEvolvR: Companion wrapper script
# accnum –> fasta –> deltablast -> edirect -> blastclust -> iprscan/rpsblast -> cleanup w/ lineages!

# Created: 2020.07.09
# Last modified: 2023.07.09
# Authors: Lo Sosinski, Janani Ravi, Jake Krol

############
## TORQUE ##
############
#PBS -l nodes=1:ppn=1		# number of nodes requested, specify wall time (FA: originally 10)

## !! NOTES FOR LS ##
## add flags/help function after finishing script and/or gencontext
## talk to Sam about how this works with Pins package

# inputs
# 	text file containing paths to individual fasta files, commonly called 'input.txt',
# 		to analyze (DO_BLAST=T)
# 	OR the interproscan query TSV file (DO_BLAST=F && IS_QUERY=T)

# usage
# option 1: homology search analysis
# 	qsub /data/research/jravilab/molevol_scripts/upstream_scripts/00_wrapper_full.sb -F 'input.txt F T <blast_database> <blast_hits> <blast_evalue>'
# option 2: ipr query tsv analysis only
# 	qsub /data/research/jravilab/molevol_scripts/upstream_scripts/00_wrapper_full.sb -F 'xxxxxx_ipr_query.tsv T F'

## change output directory based on user input
OUTPATH="${SLURM_SUBMIT_DIR}"
cd ${OUTPATH}

# Location of databases/dependencies 
export BLASTDB=/data/common_data/blastdb/v6
export BLASTMAT=/opt/software/BLAST/2.2.26/data
export INTERPRO=/opt/software/iprscan/5.47.82.0-Python3/data:/data/common_data/iprscan:/var/interproscan/data:$INTERPRO
#export SIGNALP=/var/interproscan/bin/signalp/4.1
export NCBI_API_KEY=YOUR_KEY_HERE

start=$SECONDS
START_DT=$(date '+%d/%m/%Y-%H:%M:%S')

#total=$(( ((3*60)+50)*60 ))			## total time the job can take in seconds, this should match your SBATCH line above
#maxtime=$(( 120*60 )) 				## maximum time to process one input, need to do some experimenting with your inputs
### args
# required
# $1 should be either input.txt when DO_BLAST=T and IS_QUERY=F; else: query_data/query_data.iprscan.tsv
INPUTPATHS_LIST="${1?'ERROR parameter INPUTPATHS_LIST [str] unspecified'}" 
IS_QUERY="${2?'ERROR parameter IS_QUERY [T/F] unspecified'}"
DO_BLAST="${3?'ERROR parameter DO_BLAST [T/F] unspecified'}"
# optional
DB="${4}" 
NHITS="${5}"
EVAL="${6}"
if [ "$IS_QUERY" = "T" ]; then
	PREFIX="query_data"
	OUTDIR=${OUTPATH}/${PREFIX}
	mkdir ${OUTDIR}
	cp ${INPUTPATHS_LIST} ${OUTDIR}/${PREFIX}.iprscan.tsv
	cd ${OUTDIR} || exit
	cp ../seqs.fa ${OUTDIR}/${PREFIX}.all_accnums.fa
	cp ../accs.txt ${OUTDIR}/${PREFIX}.all_accnums.txt
	## ACC2INFO ##
	acc2info_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/acc2info.sh ${OUTDIR}/${PREFIX}.all_accnums.txt $PREFIX $OUTDIR
	acc2info_dur=$(( $SECONDS - $acc2info_start ))
  	## IPR2LIN ##
	ipr2lin_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.4_ipr2lin.R ${OUTDIR}/${PREFIX}.iprscan.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX
	ipr2lin_dur=$(( $SECONDS - $ipr2lin_start ))
  	## IPR2DA ##
	ipr2da_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/05a_ipr2da.R ${OUTDIR}/${PREFIX}.iprscan_cln.tsv ${PREFIX} ${OUTDIR}/${PREFIX}.acc2info.tsv
	ipr2da_dur=$(( $SECONDS - $ipr2da_start ))
fi
if [ "$DO_BLAST" = "T" ] && [ "$IS_QUERY" = "F" ]
then
# Handle homolog data
	# get the sequence path for this batch iteration
	FASTA_PATH=$(sed -n "${SLURM_ARRAY_TASK_ID}"p "${INPUTPATHS_LIST}")
	# rm full path, get just the seq filename
	FASTA=$(basename ${FASTA_PATH})
	# rm suffix and set var for output paths
   	PREFIX=$(echo "${FASTA%%.faa}")
	# create output subdir for this seq
   	OUTDIR=${OUTPATH}/${PREFIX}_ipr
    mkdir ${OUTDIR}
	# cp seq to output subdir
    cp ${FASTA} ${OUTDIR}
    cd ${OUTDIR} || exit
    ## DELTABLAST ##
	db_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/01.1_deltablast.sh  ${FASTA} $PREFIX $OUTDIR $DB $NHITS $EVAL
	db_dur=$(( $SECONDS - $db_start ))

    ## ACC2FA -- getting fasta FILES for deltablast output(s)
	acc2fa_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/02_acc2fa.sh ${OUTDIR}/${PREFIX}.dblast.tsv $PREFIX $OUTDIR
	acc2fa_dur=$(( $SECONDS - $acc2fa_start ))

	## ACC2INFO ##
	acc2info_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/acc2info.sh ${OUTDIR}/${PREFIX}.all_accnums.txt $PREFIX $OUTDIR
	acc2info_dur=$(( $SECONDS - $acc2info_start ))

	## BLAST RESULT CLEANUP ##
	db_cln_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.2_cleanup_blast.R ${OUTDIR}/${PREFIX}.dblast.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX F
	db_cln_dur=$(( $SECONDS - $db_cln_start ))
    ## BLASTCLUST ##
	bclust_start=$SECONDS
	bash /data/research/jravilab/molevol_scripts/upstream_scripts/03.1_blastclust.sh ${OUTDIR}/${PREFIX}.all_accnums.fa $PREFIX $OUTDIR
	bclust_dur=$(( $SECONDS - $bclust_start ))

	## CLUST2TABLE
	c2t_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/03.2_clust2table.R ${OUTDIR}/${PREFIX}.bclust.L60S80.tsv ${OUTDIR}/${PREFIX}.blast.cln.tsv
	c2t_dur=$(( $SECONDS - $c2t_start ))

	## INTERPROSCAN ##
	## add second run for original protein, too
	ipr_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/04a_iprscan.sh ${OUTDIR}/${PREFIX}.all_accnums.fa ${PREFIX} ${OUTDIR}
	ipr_dur=$(( $SECONDS - $ipr_start ))
  	## IPR2LIN ##
	#Append colnames to beginning of ipr file
	sed -i '1s/^/AccNum\tSeqMD5Digest\tSLength\tAnalysis\tDB.ID\tSignDesc\tStartLoc\tStopLoc\tScore\tStatus\tRunDate\tIPRAcc\tIPRDesc\n/' ${OUTDIR}/${PREFIX}.iprscan.tsv
	ipr2lin_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.4_ipr2lin.R ${OUTDIR}/${PREFIX}.iprscan.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX
	ipr2lin_dur=$(( $SECONDS - $ipr2lin_start ))

	## IPR2DA ##
	ipr2da_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/05a_ipr2da.R ${OUTDIR}/${PREFIX}.iprscan_cln.tsv ${PREFIX} ${OUTDIR}/${PREFIX}.cln.clust.tsv
	ipr2da_dur=$(( $SECONDS - $ipr2da_start ))
fi

## Figure out how long the entire script took to run
dur=$(( $SECONDS - $start ))
printf "\nTotal run time: $dur\n"
STOP_DT=$(date '+%d/%m/%Y-%H:%M:%S')

## Add benchmarking times to logfile
RUNTIMES_ROW="${START_DT:-NA}\t${STOP_DT:-NA}\t${PREFIX:-NA}\t${db_dur:-NA}\t"
RUNTIMES_ROW+="${acc2info_dur:-NA}\t${db_cln_dur:-NA}\t${acc2fa_dur:-NA}\t"
RUNTIMES_ROW+="${bclust_dur:-NA}\t${c2t_dur:-NA}\t${ipr_dur:-NA}\t${ipr2lin_dur:-NA}\t"
RUNTIMES_ROW+="${ipr2da_dur:-NA}\t${dur:-NA}\n"
printf "${RUNTIMES_ROW}" >> ${OUTPATH}/logfile.tsv

## And how much time is left
#timeleft=$(( $total - $dur ))

	## If there's a chance we get a long input to process, then
	## resubmit this job, then kill this job
	#     if [ ${timeleft} -lt ${maxtime} ]; then
	#       sbatch --array=${SLURM_ARRAY_TASK_ID} $0
	#      scancel ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}
	#  fi
	#
        
# use logfile records to count how many jobs there are
NUM_RUNS=$(wc -l ${OUTPATH}/logfile.tsv | awk -F " " '{print $1}')
# minus 1 for the tsv column header line
((NUM_RUNS-=1))

# input.txt file means that the job sumbmission does
# include homology search and we have to treat the job 
# progress tracking accordingly; see submit_ipr() from 00_submit_full.R
if [ -e "${OUTPATH}"/input.txt ]; then
	# count number of sequences that will undergo homology search
	# and full analysis
	TOTAL_RUNS=$(wc -l ${OUTPATH}/input.txt | awk -F " " '{print $1}')
	# add one for the query job
	((TOTAL_RUNS+=1))
else
	# for no homology search, only a single query job will run
	TOTAL_RUNS=1
fi
# test if all jobs have been completed
if [ "${TOTAL_RUNS}" = "${NUM_RUNS}" ]; then
	touch ../done.txt # working dir is one level below main job output dir
fi

echo "${NUM_RUNS} / ${TOTAL_RUNS} jobs completed" > ../status.txt
