#!/bin/bash

# MolEvolvR: Companion wrapper script
# accnum –> fasta –> deltablast -> edirect -> blastclust -> iprscan/rpsblast -> cleanup w/ lineages!

# Created: 2020.07.09
# Last modified: 2023.07.09
# Authors: Lauren Sosinski, Janani Ravi, Jake Krol

# Input type: text file containing paths to individual fasta files to analyze (DO_BLAST=T)
# 	OR the interproscan query TSV file (DO_BLAST=F)

############
## TORQUE ##
############
#PBS -l nodes=1:ppn=1		# number of nodes requested, specify wall time (FA: originally 10)
#PBS -m abe			# email notifications for job
#PBS -M=falquaddoomi@gmail.com	# user email; (FA: originally jravilab.msu@gmail.com)
#PBS -N molevol_analysis	# name of job being run

## !! NOTES FOR LS ##
## add flags/help function after finishing script and/or gencontext
## talk to Sam about how this works with Pins package

## USER INPUTS

## USAGE
## qsub /data/research/jravilab/molevol_scripts/upstream_scripts/00_wrapper_full.sb -F "input.txt F"
## qsub /data/research/jravilab/molevol_scripts/upstream_scripts/00_wrapper_full.sb -F "example_blastp.csv T"

# helper functions
function exit_w_msg () {
    echo -e "*** error: $1"
    exit ${2:-1} # exit code default is 1, can be overwridden by $2
}
function print_header () {
    # use to organize stdout
    printf "\n"
    echo "$1"
    for i in {1..80}; do
        printf "="
    done
    printf "\n"
}

print_header "BEGIN $0 for job: ${SLURM_JOB_ID}"
## change output directory based on user input
OUTPATH="${SLURM_SUBMIT_DIR}"
cd ${OUTPATH}


# Location of databases/dependencies 
export BLASTDB=/data/common_data/blastdb/v6
export BLASTMAT=/opt/software/BLAST/2.2.26/data
export INTERPRO=/opt/software/iprscan/5.47.82.0-Python3/data:/data/common_data/iprscan:/var/interproscan/data:$INTERPRO
export SIGNALP=/var/interproscan/bin/signalp/4.1
export NCBI_API_KEY=YOUR_KEY_HERE

start=$SECONDS 					## get current time
START_DT=$(date '+%d/%m/%Y-%H:%M:%S')

#total=$(( ((3*60)+50)*60 ))			## total time the job can take in seconds, this should match your SBATCH line above
#maxtime=$(( 120*60 )) 				## maximum time to process one input, need to do some experimenting with your inputs
INPUTPATHS_LIST=$1 # this argument passed here is either accs.txt when DO_BLAST = T; else: query_data/query_data.iprscan.tsv
IS_QUERY=$2
DO_BLAST=$3
DB=$4
NHITS=$5
EVAL=$6
if [ "$IS_QUERY" = "T" ]; then
	PREFIX="query_data"
	OUTDIR=${OUTPATH}/${PREFIX}
	mkdir ${OUTDIR}				## make the directory
	cp ${INPUTPATHS_LIST} ${OUTDIR}/${PREFIX}.iprscan.tsv
	cd ${OUTDIR} || exit
	cp ../seqs.fa ${OUTDIR}/${PREFIX}.all_accnums.fa
	cp ../accs.txt ${OUTDIR}/${PREFIX}.all_accnums.txt
	## ACC2INFO ##
	acc2info_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/acc2info.sh ${OUTDIR}/${PREFIX}.all_accnums.txt $PREFIX $OUTDIR
	acc2info_dur=$(( $SECONDS - $acc2info_start ))
  	## IPR2LIN ##
	ipr2lin_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.4_ipr2lin.R ${OUTDIR}/${PREFIX}.iprscan.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX
	ipr2lin_dur=$(( $SECONDS - $ipr2lin_start ))
  	## IPR2DA ##
	ipr2da_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/05a_ipr2da.R ${OUTDIR}/${PREFIX}.iprscan_cln.tsv ${PREFIX} ${OUTDIR}/${PREFIX}.acc2info.tsv
	ipr2da_dur=$(( $SECONDS - $ipr2da_start ))
fi
if [ "$DO_BLAST" = "T" ] && [ "$IS_QUERY" = "F" ]
then
# Handle homolog data
	# get the sequence path for this batch iteration
	FASTA_PATH=$(sed -n "${SLURM_ARRAY_TASK_ID}"p "${INPUTPATHS_LIST}")
	# rm full path, get just the seq filename
	FASTA=$(basename ${FASTA_PATH})
	print_header "homology search for ${FASTA}; SLURM_ID=${SLURM_ARRAY_TASK_ID} starting at ${START_DT}"
	# rm suffix and set var for output paths
   	PREFIX=$(echo "${FASTA%%.faa}")
	# create output subdir for this seq
   	OUTDIR=${OUTPATH}/${PREFIX}_ipr
    mkdir ${OUTDIR}
	# cp seq to output subdir
    cp ${FASTA} ${OUTDIR}
    cd ${OUTDIR} || exit
    ## DELTABLAST ##
	db_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/01.1_deltablast.sh  ${FASTA} $PREFIX $OUTDIR $DB $NHITS $EVAL
	db_dur=$(( $SECONDS - $db_start ))

    ## ACC2FA -- getting fasta FILES for deltablast output(s)
	acc2fa_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/02_acc2fa.sh ${OUTDIR}/${PREFIX}.dblast.tsv $PREFIX $OUTDIR
	acc2fa_dur=$(( $SECONDS - $acc2fa_start ))

	## ACC2INFO ##
	acc2info_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/acc2info.sh ${OUTDIR}/${PREFIX}.all_accnums.txt $PREFIX $OUTDIR
	acc2info_dur=$(( $SECONDS - $acc2info_start ))

	## BLAST RESULT CLEANUP ##
	db_cln_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.2_cleanup_blast.R ${OUTDIR}/${PREFIX}.dblast.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX F
	db_cln_dur=$(( $SECONDS - $db_cln_start ))
    ## BLASTCLUST ##
	bclust_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/03.1_blastclust.sh ${OUTDIR}/${PREFIX}.all_accnums.fa $PREFIX $OUTDIR
	bclust_dur=$(( $SECONDS - $bclust_start ))

	## CLUST2TABLE
	c2t_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/03.2_clust2table.R ${OUTDIR}/${PREFIX}.bclust.L60S80.tsv ${OUTDIR}/${PREFIX}.blast.cln.tsv
	c2t_dur=$(( $SECONDS - $c2t_start ))

	## INTERPROSCAN ##
	## add second run for original protein, too
	ipr_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/04a_iprscan.sh ${OUTDIR}/${PREFIX}.all_accnums.fa ${PREFIX} ${OUTDIR}
	ipr_dur=$(( $SECONDS - $ipr_start ))
  	## IPR2LIN ##
	#Append colnames to beginning of ipr file
	sed -i '1s/^/AccNum\tSeqMD5Digest\tSLength\tAnalysis\tDB.ID\tSignDesc\tStartLoc\tStopLoc\tScore\tStatus\tRunDate\tIPRAcc\tIPRDesc\n/' ${OUTDIR}/${PREFIX}.iprscan.tsv
	ipr2lin_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.4_ipr2lin.R ${OUTDIR}/${PREFIX}.iprscan.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX
	ipr2lin_dur=$(( $SECONDS - $ipr2lin_start ))

	## IPR2DA ##
	ipr2da_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/05a_ipr2da.R ${OUTDIR}/${PREFIX}.iprscan_cln.tsv ${PREFIX} ${OUTDIR}/${PREFIX}.cln.clust.tsv
	ipr2da_dur=$(( $SECONDS - $ipr2da_start ))
fi

## Figure out how long the entire script took to run
dur=$(( $SECONDS - $start ))
printf "\nTotal run time: $dur\n"
STOP_DT=$(date '+%d/%m/%Y-%H:%M:%S')

## Add benchmarking times to logfile
printf "${START_DT}\t${STOP_DT}\t${PREFIX}\t${db_dur}\t${acc2info_dur}\t${db_cln_dur}\t${acc2fa_dur}\t${bclust_dur}\t${c2t_dur}\t${ipr_dur}\t${ipr2lin_dur}\t${ipr2da_dur}\t${dur}\n" >> ${OUTPATH}/logfile.tsv

## And how much time is left
#timeleft=$(( $total - $dur ))

	## If there's a chance we get a long input to process, then
	## resubmit this job, then kill this job
	#     if [ ${timeleft} -lt ${maxtime} ]; then
	#       sbatch --array=${SLURM_ARRAY_TASK_ID} $0
	#      scancel ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}
	#  fi
	#
        
# use logfile records to count how many jobs there are
NUM_RUNS=$(wc -l ${OUTPATH}/logfile.tsv | awk -F " " '{print $1}')
# minus 1 for the tsv column header line
((NUM_RUNS-=1))

# accs.txt file means that the job sumbmission does NOT 
# include homology search and we have to treat the job 
# progress tracking accordingly; see submit_ipr() from 00_submit_full.R
if [ ! -e "${OUT_PATH}"/accs.txt ]; then
	TOTAL_RUNS=$(wc -l ${OUTPATH}/accs.txt | awk -F " " '{print $1}')
	((TOTAL_RUNS+=1))
else
	TOTAL_RUNS=1
fi
# test if all jobs have been completed
if [ "${TOTAL_RUNS}" = "${NUM_RUNS}" ]; then
	touch ../done.txt # working dir is one level below main job output dir
fi

echo "${NUM_RUNS} / ${TOTAL_RUNS} jobs completed" > ../status.txt
print_header "END $0 for job: ${SLURM_JOB_ID}"
