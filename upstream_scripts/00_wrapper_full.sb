#!/bin/bash

# MolEvolvR: Companion wrapper script
# accnum –> fasta –> deltablast -> edirect -> blastclust -> iprscan/rpsblast -> cleanup w/ lineages!

# Created: 2020.07.09
# Last modified: 2023.07.06
# Authors: Lo Sosinski, Janani Ravi, Joe Burke, Jake Krol
# On GitHub: currently in jravilab/molevol_scripts/upstream_scripts

# Input type: text file containing paths to individual fasta files to analyze

############
## TORQUE ##
############
#PBS -l nodes=1:ppn=1		# number of nodes requested, specify wall time (FA: originally 10)

## !! NOTES FOR LS ##
## add flags/help function after finishing script and/or gencontext
## talk to Sam about how this works with Pins package

## USAGE
# for query
# 	qsub -N "<job_code>_<type>" /data/research/jravilab/molevol_scripts/upstream_scripts/00_wrapper_full.sb '<seqs.fa> <database> <blast_hits> <blast_evalue> T <type>'
# for homologs
#  	qsub -N "<job_code>_<type>" -t 1-<num_seqs> /data/research/jravilab/molevol_scripts/upstream_scripts/00_wrapper_full.sb 'input.txt <blast_database> <blast_hits> <blast_evalue> F <type>''

## change output directory based on user input
OUTPATH=$SLURM_SUBMIT_DIR
cd ${OUTPATH}

## USER INPUTS
INPUTPATHS_LIST="${1?'ERROR parameter INPUTPATHS_LIST [str] unspecified'}"
DB="${2?'ERROR parameter DB [str] unspecified'}" 
NHITS="${3?'ERROR parameter NHITS [int] unspecified'}"
EVAL="${4?'ERROR parameter EVAL [float] unspecified'}"
IS_QUERY="${5?'ERROR parameter IS_QUERY [T/F] unspecified'}"
TYPE="${6?'ERROR parameter TYPE [str] unspecified'}"

# Location of databases/dependencies
export BLASTDB=/data/common_data/blastdb/v6
export BLASTMAT=/opt/software/BLAST/2.2.26/data
export INTERPRO=/opt/software/iprscan/5.47.82.0-Python3/data:/data/common_data/iprscan:/var/interproscan/data:$INTERPRO
#export SIGNALP=/var/interproscan/bin/signalp/4.1
export NCBI_API_KEY=YOUR_KEY_HERE

start=$SECONDS 					## get current time
START_DT=$(date '+%d/%m/%Y-%H:%M:%S')

#total=$(( ((3*60)+50)*60 ))			## total time the job can take in seconds, this should match your SBATCH line above
#maxtime=$(( 120*60 )) 				## maximum time to process one input, need to do some experimenting with your inputs
if [ "$IS_QUERY" = "T" ]; then
	# Handle the query data
	FILE="$INPUTPATHS_LIST"
	F=$(basename ${FILE})
	PREFIX="query_data"
	OUTDIR=${OUTPATH}/${PREFIX}
	mkdir ${OUTDIR}				## make the directory
	cat $FILE >> ${OUTDIR}/${PREFIX}.all_accnums.fa
	# use the attempted parsing of accession numbers from headers for acc2info
	tail -n +2 query-fasta_header-map.tsv | cut -f2 > parsed_accnums.txt
	if [ -e starting_accs.txt ]; then
		cp starting_accs.txt ${OUTDIR}/${PREFIX}.all_accnums.txt
	else
		cp accs.txt ${OUTDIR}/${PREFIX}.all_accnums.txt
		cp parsed_accnums.txt ${OUTDIR}/${PREFIX}.parsed_accnums.txt
	fi

	cd ${OUTDIR} || exit
	## ACC2INFO ##
	acc2info_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/acc2info.sh ${OUTDIR}/${PREFIX}.parsed_accnums.txt $PREFIX $OUTDIR
	/data/research/jravilab/molevol_scripts/upstream_scripts/substitute_accnum_for_acc2info.R \
		${OUTDIR}/${PREFIX}.acc2info.tsv \
		${OUTPATH}/query-fasta_header-map.tsv
	acc2info_dur=$(( $SECONDS - $acc2info_start ))
	cp ${OUTDIR}/${PREFIX}.acc2info.tsv ${OUTDIR}/${PREFIX}.blast.cln.tsv
else
	# Handle homolog data
	FILE=$(sed -n "${SLURM_ARRAY_TASK_ID:-1}"p "${INPUTPATHS_LIST}")
	F=$(basename ${FILE})
   	PREFIX=$(echo "${F%%.faa}")	## takes PREFIX of file
   	OUTDIR=${OUTPATH}/${PREFIX}_${TYPE}	## variable containing output filepath based PREFIX
   	printf "${PREFIX}\n"
	mkdir ${OUTDIR}				## make the directory
    cd ${OUTDIR} || exit

	## DELTABLAST ##
	db_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/01.1_deltablast.sh $FILE $PREFIX $OUTDIR $DB $NHITS $EVAL
	db_dur=$(( $SECONDS - $db_start ))

    ## ACC2FA -- getting fasta FILES for deltablast output(s)
	acc2fa_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/02_acc2fa.sh ${OUTDIR}/${PREFIX}.dblast.tsv $PREFIX $OUTDIR
	acc2fa_dur=$(( $SECONDS - $acc2fa_start ))

	## ACC2INFO ##
	acc2info_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/acc2info.sh ${OUTDIR}/${PREFIX}.all_accnums.txt $PREFIX $OUTDIR
	acc2info_dur=$(( $SECONDS - $acc2info_start ))

	## BLAST RESULT CLEANUP ##
	db_cln_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.2_cleanup_blast.R ${OUTDIR}/${PREFIX}.dblast.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX F
	db_cln_dur=$(( $SECONDS - $db_cln_start ))
fi 

sleep 30
## BLASTCLUST ##
bclust_start=$SECONDS
BLAST_FULL_CMD="bash /data/research/jravilab/molevol_scripts/upstream_scripts/03.1_blastclust.sh ${OUTDIR}/${PREFIX}.all_accnums.fa $PREFIX $OUTDIR"
echo "* Starting blastclust wrapper: ${BLAST_FULL_CMD}"
${BLAST_FULL_CMD}
bclust_dur=$(( $SECONDS - $bclust_start ))

## CLUST2TABLE
c2t_start=$SECONDS
Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/03.2_clust2table.R ${OUTDIR}/${PREFIX}.bclust.L60S80.tsv ${OUTDIR}/${PREFIX}.blast.cln.tsv
c2t_dur=$(( $SECONDS - $c2t_start ))

if [ "$TYPE" != "dblast" ] || [ "$IS_QUERY" = "T" ]; then
	## INTERPROSCAN ##
	## add second run for original protein, too
	ipr_start=$SECONDS
	sh /data/research/jravilab/molevol_scripts/upstream_scripts/04a_iprscan.sh ${OUTDIR}/${PREFIX}.all_accnums.fa ${PREFIX} ${OUTDIR}
	ipr_dur=$(( $SECONDS - $ipr_start ))

	## IPR2LIN ##
	#Append colnames to beginning of ipr file
	LINE_COUNT=$(wc -l ${OUTDIR}/${PREFIX}.iprscan.tsv | grep -Eo "^[[:digit:]]+")
	if [ $LINE_COUNT -eq 0 ]; then
		printf "AccNum\tSeqMD5Digest\tSLength\tAnalysis\tDB.ID\tSignDesc\tStartLoc\tStopLoc\tScore\tStatus\tRunDate\tIPRAcc\tIPRDesc\n" > ${OUTDIR}/${PREFIX}.iprscan.tsv
	else
		sed -i '1s/^/AccNum\tSeqMD5Digest\tSLength\tAnalysis\tDB.ID\tSignDesc\tStartLoc\tStopLoc\tScore\tStatus\tRunDate\tIPRAcc\tIPRDesc\n/' ${OUTDIR}/${PREFIX}.iprscan.tsv
	fi
	ipr2lin_start=$SECONDS
	Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/01.4_ipr2lin.R ${OUTDIR}/${PREFIX}.iprscan.tsv ${OUTDIR}/${PREFIX}.acc2info.tsv $PREFIX
	ipr2lin_dur=$(( $SECONDS - $ipr2lin_start ))
	if [ "$IS_QUERY" = "T" ]; then
		## IPR2DA ##
		ipr2da_start=$SECONDS
		Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/05a_ipr2da.R ${OUTDIR}/${PREFIX}.iprscan_cln.tsv ${PREFIX} NA
		ipr2da_dur=$(( $SECONDS - $ipr2da_start ))
		cp ${OUTDIR}/${PREFIX}.ipr_domarch.tsv ${OUTDIR}/${PREFIX}.full_analysis.tsv
	else
		## IPR2DA ##
		ipr2da_start=$SECONDS
		Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/05a_ipr2da.R ${OUTDIR}/${PREFIX}.iprscan_cln.tsv ${PREFIX} ${OUTDIR}/${PREFIX}.cln.clust.tsv
		ipr2da_dur=$(( $SECONDS - $ipr2da_start ))
	fi
	## RPSBLAST ##
	rps_start=$SECONDS
	#sh /data/research/jravilab/molevol_scripts/upstream_scripts/04b_rpsblast.sh ${OUTDIR}/${PREFIX}.all_accnums.fa ${PREFIX} ${OUTDIR}
	rps_dur=$(( $SECONDS - $rps_start))

	## RPS2DA ##
	rps2da_start=$SECONDS
	#Rscript /data/research/jravilab/molevol_scripts/upstream_scripts/05b_rps2da.R ${OUTDIR}/${PREFIX}.rps.out ${OUTDIR}/${PREFIX}.cln.clust.ipr.tsv ${PREFIX}
	rps2da_dur=$(( $SECONDS - $rps2da_start ))
fi
cp ${FILE} ${OUTDIR}	# copy fasta file to output directory

## Figure out how long the entire script took to run
dur=$(( $SECONDS - $start ))
printf "\nTotal run time: $dur\n"
STOP_DT=$(date '+%d/%m/%Y-%H:%M:%S')

## Add benchmarking times to logfile
RUNTIMES_ROW="${START_DT:-NA}\t${STOP_DT:-NA}\t${PREFIX:-NA}\t${db_dur:-NA}\t${acc2info_dur:-NA}\t"
RUNTIMES_ROW+="${db_cln_dur:-NA}\t${acc2fa_dur:-NA}\t${bclust_dur:-NA}\t${c2t_dur:-NA}\t"
RUNTIMES_ROW+="${ipr_dur:-NA}\t${ipr2lin_dur:-NA}\t${ipr2da_dur:-NA}\t${dur:-NA}\n"
printf "${RUNTIMES_ROW}" >> ${OUTPATH}/logfile.tsv

	## And how much time is left
	#timeleft=$(( $total - $dur ))

        ## If there's a chance we get a long input to process, then
        ## resubmit this job, then kill this job
#  if [ ${timeleft} -lt ${maxtime} ]; then
#  	sbatch --array=${SLURM_ARRAY_TASK_ID} $0
#  	scancel ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}
        #  fi
NUM_RUNS=$(wc -l "${OUTPATH}"/logfile.tsv | grep -Eo "^[[:digit:]]+")
((NUM_RUNS-=1))
if [ -e "${OUTPATH}"/accs.txt ]; then
TOTAL_RUNS=$(wc -l "${OUTPATH}"/accs.txt | grep -Eo "^[[:digit:]]+")
((TOTAL_RUNS+=1))
else
TOTAL_RUNS=1
fi

# handling phylogenetic analysis where input.txt is never created
if [ ! -e "${OUTPATH}"/input.txt ]; then
TOTAL_RUNS=1
fi

if [ $TOTAL_RUNS = $NUM_RUNS ]; then
  touch ../done.txt
fi

echo "${NUM_RUNS} / ${TOTAL_RUNS} jobs completed" > ../status.txt


# FA: disabled b/c docker's handling perms
# setfacl -R -m group:shiny:r-x ${OUTDIR}
